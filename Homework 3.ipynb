{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9584177 1.6891866]\n",
      "0.00071549066\n",
      "p= 28.1 & p model= 28.824099527405245 Error -0.7240995274052437\n",
      "p= 34.4 & p model= 34.64430983581709 Error -0.24430983581709143\n",
      "p= 36.7 & p model= 36.452964624211475 Error 0.24703537578852774\n",
      "p= 36.9 & p model= 36.86731249815574 Error 0.03268750184425784\n",
      "p= 36.8 & p model= 36.87400708532342 Error -0.07400708532342293\n",
      "p= 36.7 & p model= 36.74983136261855 Error -0.049831362618547814\n",
      "p= 36.5 & p model= 36.390446913894365 Error 0.10955308610563463\n",
      "p= 35.4 & p model= 35.38482080704233 Error 0.015179192957667453\n",
      "p= 32.9 & p model= 32.94778400610805 Error -0.04778400610805278\n",
      "p= 27.7 & p model= 27.730002077430925 Error -0.03000207743092531\n",
      "p= 17.5 & p model= 17.47325208459706 Error 0.026747915402939526\n"
     ]
    }
   ],
   "source": [
    "# Here is a code for gradient descent without line search\n",
    "import numpy as np\n",
    "import math as m\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "from line_search_bt import *\n",
    "T=20\n",
    "\n",
    "a_water=np.array([8.07131,1730.63,233.426])\n",
    "a_dioxane=np.array([7.43155,1554.679,240.337])\n",
    "\n",
    "x1=np.array([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "x2=1-x1\n",
    "\n",
    "p=np.array([28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5])\n",
    "\n",
    "def p_specific(a,T):\n",
    "   return 10**(a[0]-((a[1])/(T+a[2])))\n",
    "\n",
    "pwater=((p_specific(a_water,T)))\n",
    "pdioxane=((p_specific(a_dioxane,T)))\n",
    "\n",
    "x = Variable(t.tensor([1.0, 1.0]), requires_grad=True)\n",
    "\n",
    "# Fix the step size\n",
    "a = 0.001\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(100):  # TODO: change the termination criterion\n",
    "    for i in range(0,len(x1)):\n",
    "        \n",
    "        loss = (((x1[i]*pwater*t.exp(x[0]*((x[1]*x2[i])/(x[0]*x1[i]+x[1]*x2[i]))**2)) + (x2[i]*pdioxane*t.exp( x[1]*((x[0]*x1[i])/(x[0]*x1[i]+x[1]*x2[i]))**2))) - p[i])**2\n",
    "    \n",
    "        loss.backward()\n",
    "    x.grad.numpy()\n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        \n",
    "        x -= a * x.grad\n",
    "        \n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        x.grad.zero_()\n",
    "        \n",
    "print(x.data.numpy())\n",
    "print(loss.data.numpy())\n",
    "\n",
    "for i in range(0,len(p)):\n",
    "   pcheck = ((x1[i]*pwater*m.exp(x[0]*((x[1]*x2[i])/(x[0]*x1[i]+x[1]*x2[i]))**2)) + (x2[i]*pdioxane*m.exp( x[1]*((x[0]*x1[i])/(x[0]*x1[i]+x[1]*x2[i]))**2)))\n",
    "   print(\"p=\",p[i], \"&\", \"p model=\",pcheck, \"Error\" , p[i]-pcheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_28488/3315217098.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\cwhit\\AppData\\Local\\Temp/ipykernel_28488/3315217098.py\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    def bayesian_optimization(n_iters,loss(x), xp, yp):\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import sklearn.gaussian_process as gp\n",
    "def func(x):\n",
    "    return (4-2.1*x[0]**+(x[0]**4/3))*x[0]**2+x[0]*x[1]+(-4+4*x[1]**2)*x[1]**2\n",
    "niters=100\n",
    "x=[0,0]\n",
    "\n",
    "def bayesian_optimization(n_iters,func(x), xp, yp):\n",
    "\n",
    "      kernel = gp.kernels.Matern()\n",
    "  model = gp.GaussianProcessRegressor(kernel=kernel,alpha=1e-4,n_restarts_optimizer=10,normalize_y=True)\n",
    "                               \n",
    "  for i in range(n_iters):\n",
    "    # Update our belief of the loss function\n",
    "    model.fit(xp, yp)\n",
    "\n",
    "    # sample_next_hyperparameter is a method that computes the arg\n",
    "    # max of the acquisition function\n",
    "    next_sample = sample_next_hyperparameter(model, yp)\n",
    "\n",
    "    # Evaluate the loss for the new hyperparameters\n",
    "    next_loss = sample_loss(next_sample)\n",
    "\n",
    "    # Update xp and yp\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "648d7d48fcae17006cede8a07b6574169a1e230de14379267f6ccaecb2e08bc1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
